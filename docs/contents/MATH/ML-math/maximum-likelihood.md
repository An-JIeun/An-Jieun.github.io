---
title : 최대우도추정
description: 가우시안 확률 밀도 함수 매개변수 추정법
keywords : '"머신러닝, 확률변수, 가우시안, 최대우도"'
url : "MATH/ML-math/maximum-likelihood.html"
---

# 최대우도추정

로지스틱 회귀와 선형 회귀에서는 입력값에 대한 출력값을 예측하는 선형함수를 찾거나, 입력값에 따라 잘 분류할 수 있는 로직을 구성하는 매개변수를 찾는 것이 목표였다. 아무튼 회귀를 쓴다는 것은 데이터의 분포형태를 예상할 수 없는 경우에 사용한다. 

하지만, 특정 자연현상에 대한 측정값이라면, 대체로 가우시안 분포를 가질 것이라 예상할 수 있고, 실제로 대부분은 가우시안 확률 분포를 띄고 있다. 따라서, 이처럼 확률 밀도 함수(PDF)를 명확하게 가정할 수 있는 상황이라면, 확률 밀도 함수에 따른 분포를 조정하는 매개변수를 찾음으로써, 데이터를 대표하는 확률 모델을 찾아낼 수 있다.

:::info 💡Average vs Mean vs Expectation

Average와 Mean, Expectation 모두 '평균'으로 인식한다. (Expectation은 기댓값이라고 하긴 하는데 '모집단의 평균'과 의미상 유사하다고 배우기 때문에 포함했다) 하지만 세 단어는 모두 의미적으로 차이가 난다.

**Average** | Average는 $\frac{\sum_{i=1}^{N}x_i}{N}$을 의미한다. 값의 분포와는 관계 없이, 산술적으로 전체 값의 합을 값의 개수로 나눈 값을 의미한다.

**Mean** | Mean은 가우시안 분포를 갖는 데이터세트에서의 평균 값을 의미한다. 가우시안 분포는 Mean과 Average, 그리고 중앙값(Median)이 모두 동일한 분포이다.

**Expectation** | Expectation은 확률값과 그 확률 값을 갖는 값의 곱을 모두 더한 값을 의미한다. 어떤 사건에 대한 확률 분포가 있을 때, 그 사건으로 인해 발생하는 값들에 대한 기대치를 의미한다. 수식으로는 $\sum_{i=1}^{N}x_i*p(x_i)$ 로 나타낼 수 있다.
:::

확률 모델에서 분포 형태를 조정하는 매개변수는 평균과 분산이다. 즉, 최대우도법에서는 확률의 총곱인 우도가 최대가 되는 평균($\mu$)과 분산($\sigma$)을 찾는 것이 목표이다.

## 가우시안 확률 분포 함수에서의 매개변수 추정

출구조사를 예로 들어 보자. 후보자가 두 명이고 이미 투표는 완료되었다고 할 때, A후보자에 대한 예상 지지율을 도출 한다고 하자. 출구조사에 참여한 사람은 10만명이다. 이때 후보자 레이블은 ${C_1, C_2}$ 일 것이고, 각 레이블이 전체 데이터세트에서 갖는 비율을 ${V_1, V_2}$라고 하자.

예상 지지율을 확인하기 위해 500명에 대한 샘플링을 1000회 진행하였다고 했을 때, 각 샘플마다 각 후보자의 지지율은 조금씩 다르게 나타날 것이다.

그러다 보면, A 후보자에 대한 1000개의 샘플링 세트에서 나타난 지지율을 그래프로 표현하면 어떤 정규 분포로 나타날 것이다. 여기서 1000개의 샘플링에서 나타난 평균값과 분산을 구할 수 있고, 대체로 평균값을 예측 지지율이라고 한다. 그리고 신뢰 구간은 분산 값으로 표현한다.

이처럼 일반적인 가설 검증에서는 모집단에서 샘플을 추출하고, 샘플값에서 해당 레이블이 갖는 비율들의 평균과 분산을 통해 레이블이 갖는 비율을 추정한다. 

하지만,최대우도법은 가설 검정과는 조금 다르다. 먼저 최대우도법에서는 샘플링을 하지 않는다. 

데이터 레이블이 차지하는 비율을 정하지 않고, 해당 데이터 레이블을 갖는 데이터 값들에 대한 확률 분포 모델을 찾는 것이다. 

예를 들어, 중복값이 거의 없는 어떤 연속적인 값들에 대한 분포 모델을 도출할 때 아래의 식을 통해 최적의 평균과 분산 값을 구할 수 있다. 

$$
p(D|\Theta) = \prod_{k=0}^{N-1}p(x_k|\Theta),\space \Theta = (\mu,\sigma)
$$

가우시안 확률 밀도 함수는 아래와 같다. (해당 식에서는 표준편차를 $\sigma$로 표현했으나, 뒤이어 다룰 내용에서는 편의상 $\sigma$를 분산으로 표기한다.)

$$
N(x) = \frac{1}{\sigma \sqrt{2\pi}}exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$


시그모이드 함수와 마찬가지로, 자연상수 $e$가 존재하는 함수이므로, 미분 연산을 위해서는 시그모이드 함수와 마찬가지로 $ln$을 취해야 한다.

여기서, $X$가 d차원인 경우, 가우시안 분포 모델임을 알고 분산 세트는 아는데 평균을 모를 때, 평균에 대한 추정은 다음의 과정을 거쳐 구할 수 있다.

각 차원에 대한 평균에 대한 d차원 벡터 $M$과 d x d 행렬을 갖는 분산벡터 $D$를 구하는 것이 목표이다. 분산행렬의 각 행렬은 두 개의 차원 요소 쌍에서 갖는 데이터의 분산을 나타낸다.

$$
M = [\mu_1,\mu_2,\mu_3, ... ,\mu_d ]^T
$$

$$
D = 
\begin{bmatrix}
\sigma_{1,1} && \sigma_{1,2} && ... && \sigma_{1,d-1} && \sigma_{1,d} \\
\sigma_{2,1} && \sigma_{2,2} && ... && \sigma_{2,d-1} && \sigma_{2,d} \\
\space&& && ... && \space \\
\sigma_{d,1} && \sigma_{d,2} && ... && \sigma_{d,d-1} && \sigma_{d,d} 
\end{bmatrix}
$$

이러한 경우애서의 가우시안 확률 밀도 함수 식은 아래와 같이 표현할 수 있다.

$$
p(D|\Theta) =\prod_{i=0}^{N-1} \frac{1}{{{(2\pi)}}^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}}exp(- \frac{1}{2}(X^{(i)}-M)^T\Sigma^{-1}(X^{(i)}-M))
$$

### 다차원 데이터에서 분산 행렬이 고정 상수일 때
분산값이 임의의 고정된 상수라고 할 때, $\mu$에 대해 미분하여 최적의 $\mu$를 찾는 방법은 다음과 같다.

> **1. 미분에 앞서, exponential을 미분가능한 형태로 바꾸기 위해 로그를 취한다.**

$$
ln\space p(D|\Theta) =  (\frac{1}{{{(2\pi)}}^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}})\sum_{i=1}^{N-1} (- \frac{1}{2}(X^{(i)}-M)^T\Sigma^{-1}(X^{(i)}-M)) 
$$

$$
= (\frac{1}{{{(2\pi)}}^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}})\sum_{i=1}^{N-1} (- \frac{\Sigma^{-1}}{2}(X^{(i)}-M)^T(X^{(i)}-M)) 
$$

> **2. 위의 식에 $\mu_i$에 대한 편미분 벡터를 곱하여 미분한다.**
- $\triangledown_{\mu}$벡터
$$
M=[\mu_1, ..., \mu_d],\space\triangledown_{\mu} = [\frac{\partial}{\partial\mu_1},\frac{\partial}{\partial\mu_2},...,\frac{\partial}{\partial\mu_d}]
$$


$$
\triangledown_{\mu}ln\space p(D|\Theta) = \triangledown_{\mu}(\frac{1}{{{(2\pi)}}^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}})\sum_{i=1}^{N-1} (- \frac{\Sigma^{-1}}{2}(X^{(i)}-M)^T(X^{(i)}-M)) = 0
$$

시그마 내부의 행렬곱에 대한 계산은 아래와 같다.
$$
(X^{(i)}-M)^T(X^{(i)}-M) = [(x_1^{(i)}-\mu_1)^2, ...,(x_d^{(i)}-\mu_d)^2 ]
$$

여기에 편미분벡터를 곱하면 아래와 같다.
$$
\triangledown_{\mu}(X^{(i)}-M)^T(X^{(i)}-M)  = \triangledown_{\mu}[(x_1^{(i)}-\mu_1)^2, ...,(x_d^{(i)}-\mu_d)^2 ]
$$
$$
= [(-2x_1+2\mu_1), ...., (-2x_d+2\mu_d)]
$$

이 결과를 위의 전체 식에 반영하면 다음과 같은 결과를 얻을 수 있다.

$$
= (\frac{1}{{{(2\pi)}}^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}})\sum_{i=1}^{N-1} 2*(- \frac{\Sigma^{-1}}{2})(X^{(i)}-M)
$$

분산행렬이 0이 될 수는 없으므로 $X^{(i)} - M = 0$임이 자명하다. 따라서 최종적으로는 $\sum_{i=0}^{N-1}X^{(i)}-\sum_{i=0}^{N-1}M = 0$ 이고, $\sum_{i=0}^{N-1}M = NM$으로 나타낼 수 있다.$M$에 대해 식을 정리하면 아래와 같은 결과를 얻을 수 있다.

$$
\sum_{i=0}^{N-1}X^{(i)} = NM,\space \therefore M = \frac{\sum_{i=0}^{N-1}X^{(i)}}{N}
$$

즉, 다차원이라고 하더라도 $M$의 $\mu_k$는 여전히 각 데이터 값의 총합을 총 데이터 개수로 나눈 Average와 같다는 것을 확인할 수 있다. 사실, 이렇게 복잡하게 증명해볼 필요 없이, 각 차원에 대한 평균 벡터는 각 차원별 데이터의 평균값인 것이 당연하긴 하다.

### 다차원 데이터에서 분산도 확률변수일 때 (TBD)

분산이 고정되었다고 하였을 때는 위와 같이 간단하게 평균값을 구할 수 있지만, 만일 분산값 역시 확률 변수라고 하면, 계산이 복잡해진다. 여기서부터는 공분산(Covariance) 개념이 들어간다.
보통 단차원의 데이터 세트에 대해서는 $\Theta$에 대한 미분을 쉽게 적용할 수 있다. 하지만 다차원의 경우, 미분 연산이 매우 복잡해지므로 경사하강법으로 구해야 한다.






